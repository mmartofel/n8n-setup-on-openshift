kind: Deployment
apiVersion: apps/v1
metadata:
  annotations:
    app.openshift.io/vcs-uri: 'https://github.com/mmartofel/'
  labels:
    app: llama-server
    app.kubernetes.io/component: AI
    app.kubernetes.io/name: llama-server
    app.kubernetes.io/part-of: 'LLM-SERVICES'
    app.openshift.io/runtime: llama
  name: llama-server
spec:
  replicas: 1
  selector:
    matchLabels:
      app: llama-server
  template:
    metadata:
      labels:
        app: llama-server
    spec:
      containers:
        - name: container
          image: 'quay.io/mmartofe/llama-server:latest'
          ports:
            - containerPort: 8000
              protocol: TCP
          resources:
            limits:
              cpu: '6'
              memory: 4Gi
            requests:
              cpu: '4'
              memory: 2Gi
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          imagePullPolicy: Always
      restartPolicy: Always
      terminationGracePeriodSeconds: 30
      dnsPolicy: ClusterFirst
  strategy:
    type: RollingUpdate
    rollingUpdate:
      maxUnavailable: 25%
      maxSurge: 25%
  revisionHistoryLimit: 10
  progressDeadlineSeconds: 600